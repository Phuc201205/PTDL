import streamlit as st
import pandas as pd
import numpy as np
import re
import unicodedata
import emoji
import matplotlib.pyplot as plt
import seaborn as sns
from pyvi import ViTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from imblearn.over_sampling import SMOTE
from wordcloud import WordCloud
from scipy.stats import norm
from collections import Counter
import itertools

# =============================================================================
# 1. C·∫§U H√åNH GIAO DI·ªÜN & T·ª™ ƒêI·ªÇN
# =============================================================================
st.set_page_config(
    page_title="ABSA Sentiment Analyzer",
    page_icon="üì±",
    layout="wide"
)

# CSS t√πy ch·ªânh giao di·ªán
st.markdown("""
<style>
    .stTextArea textarea {font-size: 16px;}
    .metric-card {
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 10px;
        text-align: center;
        color: white;
        font-weight: bold;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
    }
    .positive {background-color: #28a745;}
    .negative {background-color: #dc3545;}
    .neutral {background-color: #6c757d;}
    
    .overall-card {
        padding: 20px;
        border-radius: 15px;
        margin-bottom: 25px;
        text-align: center;
        color: white;
        font-size: 24px;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

ASPECTS = ['BATTERY', 'CAMERA', 'DESIGN', 'FEATURES', 'GENERAL', 'PERFORMANCE', 'PRICE', 'SCREEN', 'SER&ACC', 'STORAGE']
SENTIMENT_MAP = {1: 'Ti√™u c·ª±c', 2: 'Trung t√≠nh', 3: 'T√≠ch c·ª±c'}

STATIC_TEENCODE = {
    "mk": "m√¨nh", "mik": "m√¨nh", "mjk": "m√¨nh", "m": "m√¨nh", "t": "t√¥i", "tui": "t√¥i",
    "tao": "t√¥i", "t·ªõ": "t√¥i", "b": "b·∫°n", "bn": "b·∫°n", "shop": "c·ª≠a h√†ng", "x·ªëp": "c·ª≠a h√†ng",
    "nv": "nh√¢n vi√™n", "ship": "giao h√†ng", "shipper": "ng∆∞·ªùi giao h√†ng",
    "k": "kh√¥ng", "ko": "kh√¥ng", "kh": "kh√¥ng", "hok": "kh√¥ng", "khum": "kh√¥ng", "not": "kh√¥ng",
    "dt": "ƒëi·ªán tho·∫°i", "ƒët": "ƒëi·ªán tho·∫°i", "mb": "m√°y", "mobile": "ƒëi·ªán tho·∫°i",
    "ip": "iphone", "ss": "samsung", "sam": "samsung", "t√°o": "apple",
    "cam": "camera", "mic": "micro", "loa": "loa", "pin": "pin", "bin": "pin",
    "sac": "s·∫°c", "cap": "c√°p",
    "dc": "ƒë∆∞·ª£c", "ƒëc": "ƒë∆∞·ª£c", "dk": "ƒë∆∞·ª£c", "ok": "t·ªët", "oke": "t·ªët", "·ªïn": "t·ªët",
    "gud": "t·ªët", "good": "t·ªët", "bad": "t·ªá", "lag": "gi·∫≠t", "l√°c": "gi·∫≠t", "ƒë∆°": "ƒë·ª©ng m√°y",
    "m∆∞·ª£t": "nhanh", "nh√¨u": "nhi·ªÅu", "wa": "qu√°", "w√°": "qu√°", "mua": "mua", "xai": "x√†i",
    "app": "·ª©ng d·ª•ng", "game": "tr√≤ ch∆°i", "fb": "facebook", "mess": "tin nh·∫Øn",
    "tr": "tri·ªáu", "c·ªß": "tri·ªáu"
}

STOPWORDS = set(["b·ªã", "b·ªüi", "c·∫£", "c√°c", "c√°i", "c·∫ßn", "c√†ng", "th√¨", "l√†", "m√†"])
NEGATION_WORDS = ["kh√¥ng", "ch·∫≥ng", "ch·∫£", "ch∆∞a", "ƒë·ª´ng", "k", "ko", "kh", "n·ªè", "not", "ƒë·∫øch", "√©o"]

# =============================================================================
# 2. H√ÄM X·ª¨ L√ù TEXT (CLEANING)
# =============================================================================
def resolve_ambiguity(text):
    text = " " + text + " "
    text = re.sub(r'(\d+)\s*k\b', r'\1 ngh√¨n', text)
    text = re.sub(r'\bk\b', 'kh√¥ng', text)
    text = re.sub(r'\b(xin|g·ª≠i|t·∫°i|·ªü)\s+(dc|ƒëc)\b', r'\1 ƒë·ªãa ch·ªâ', text)
    text = re.sub(r'\b(dc|ƒëc)\b', 'ƒë∆∞·ª£c', text)
    return text.strip()

def normalize_repeated_characters(text):
    return re.sub(r'(\w)\1{2,}', r'\1', text)

def merge_negation(text):
    words = text.split()
    new_words = []
    i = 0
    while i < len(words):
        word = words[i]
        if word in NEGATION_WORDS and i < len(words) - 1:
            new_words.append(f"{word}_{words[i+1]}")
            i += 2
        else:
            new_words.append(word)
            i += 1
    return " ".join(new_words)

def clean_text_ultimate(text):
    if pd.isna(text): return ""
    text = str(text)
    text = emoji.demojize(text, delimiters=(" ", " "))
    text = unicodedata.normalize('NFC', text).lower()
    text = resolve_ambiguity(text)
    
    sorted_keys = sorted(STATIC_TEENCODE.keys(), key=len, reverse=True)
    pattern = re.compile(r'\b(' + '|'.join(map(re.escape, sorted_keys)) + r')\b')
    text = pattern.sub(lambda x: STATIC_TEENCODE[x.group()], text)
    
    text = normalize_repeated_characters(text)
    text = re.sub(r'[^\w\s_:]', ' ', text)
    text = ViTokenizer.tokenize(text)
    text = merge_negation(text)
    
    tokens = [t for t in text.split() if t not in STOPWORDS]
    return " ".join(tokens)

# =============================================================================
# 3. H√ÄM HU·∫§N LUY·ªÜN MODEL
# =============================================================================
@st.cache_resource
def train_model(uploaded_file):
    df = pd.read_csv(uploaded_file)
    
    # T√°ch nh√£n (Label Parsing)
    if 'BATTERY' not in df.columns:
        def parse_labels(row):
            res = {asp: 0 for asp in ASPECTS}
            if pd.isna(row['label']): return pd.Series(res)
            tags = row['label'].split(';')
            for tag in tags:
                tag = tag.strip().replace('{', '').replace('}', '')
                if '#' in tag:
                    parts = tag.split('#')
                    asp, sent = parts[0], parts[1] if len(parts) > 1 else None
                    if asp in ASPECTS and sent in {'Negative': 1, 'Neutral': 2, 'Positive': 3}: 
                        res[asp] = {'Negative': 1, 'Neutral': 2, 'Positive': 3}[sent]
            return pd.Series(res)
        label_df = df.apply(parse_labels, axis=1)
        df = pd.concat([df, label_df], axis=1)

    df['comment_cleaned'] = df['comment'].apply(clean_text_ultimate)
    df_clean = df.dropna(subset=['comment_cleaned'])
    df_clean = df_clean[(df_clean['comment_cleaned'].apply(lambda x: len(str(x).split())) >= 3)]

    vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 5), min_df=2, sublinear_tf=True)
    X_all_vec = vectorizer.fit_transform(df_clean['comment_cleaned'])
    models = {}

    progress_bar = st.progress(0)
    for idx, aspect in enumerate(ASPECTS):
        y = df_clean[aspect]
        try:
            sampler = SMOTE(random_state=42, k_neighbors=1)
            X_res, y_res = sampler.fit_resample(X_all_vec, y)
        except:
            X_res, y_res = X_all_vec, y
            
        svm = LinearSVC(dual='True', class_weight='balanced', random_state=42)
        lr = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)
        ensemble = VotingClassifier(estimators=[('svm', svm), ('lr', lr)], voting='hard')
        ensemble.fit(X_res, y_res)
        models[aspect] = ensemble
        progress_bar.progress((idx + 1) / len(ASPECTS))
    
    progress_bar.empty()
    return vectorizer, models, df_clean

# =============================================================================
# 4. HARD RULES V4.1
# =============================================================================
def apply_hard_rules_hybrid(text, pred_vector):
    text_lower = text.lower()
    def set_sent(asp_name, val):
        idx = ASPECTS.index(asp_name)
        pred_vector[idx] = val
    def has_kw(keywords):
        return any(kw in text_lower for kw in keywords)

    # 1. C·∫•u tr√∫c c√¢u
    contrast_words = ['tuy nhi√™n', 'nh∆∞ng m√†', 'c√≥ ƒëi·ªÅu', 'm·ªói t·ªôi', 'ƒëi·ªÉm tr·ª´', 'ti·∫øc l√†']
    for word in contrast_words:
        if word in text_lower:
            parts = text_lower.split(word)
            if len(parts) > 1:
                after_part = parts[1]
                if 'cam' in after_part or '·∫£nh' in after_part: set_sent('CAMERA', 1)
                if 'pin' in after_part: set_sent('BATTERY', 1)
                if 'm√†n' in after_part: set_sent('SCREEN', 1)
                if 'loa' in after_part: set_sent('FEATURES', 1)
                if 'n√≥ng' in after_part: set_sent('PERFORMANCE', 1)

    # 2. Domain Rules
    if has_kw(['pin', 'bin']):
        if has_kw(['tr√¢u', 'kh·ªèe', 'l√¢u', 'c·∫£ ng√†y', 'ngon', 'm·∫°nh', 't·ªët', '·ªïn', 'b·ªÅn']): set_sent('BATTERY', 3)
        if has_kw(['tu·ªôt', 't·ª•t', 'y·∫øu', 'h·∫ªo', 'nhanh h·∫øt', 's·ª•t']): set_sent('BATTERY', 1)

    if has_kw(['m√†n h√¨nh', 'm√†n']):
        if has_kw(['n√©t', 'ƒë·∫πp', 's·∫Øc', 'nh·∫°y', 'm∆∞·ª£t']): set_sent('SCREEN', 3)
        if has_kw(['r·ªó', '√°m v√†ng', 't·ªëi', 'ƒë∆°', 'lo·∫°n', 'li·ªát', 'nh√≤e']): set_sent('SCREEN', 1)

    if has_kw(['cam', '·∫£nh', 'ch·ª•p', 'selfie', 'quay']):
        if has_kw(['n√©t', 'ƒë·∫πp', '·∫£o', 'ngon', 'r√µ', 'xu·∫•t s·∫Øc']): set_sent('CAMERA', 3)
        elif has_kw(['m·ªù', 'x·∫•u', 'b·ªÉ', 'nh√≤e', 't·ªá', 'k√©m', 'rung']): set_sent('CAMERA', 1)

    if has_kw(['n√≥ng', '·∫•m m√°y', 't·ªèa nhi·ªát']): set_sent('PERFORMANCE', 1)
    if has_kw(['game', 'li√™n qu√¢n', 'pubg', 't√°c v·ª•', 'm√°y']):
        if has_kw(['m∆∞·ª£t', 'ngon', 'ph√™', 'nhanh', 'm·∫°nh']): set_sent('PERFORMANCE', 3)
        if has_kw(['lag', 'gi·∫≠t', 'kh·ª±ng', 'ƒë·ª©ng', 'vƒÉng']): set_sent('PERFORMANCE', 1)
    if has_kw(['lag', 'gi·∫≠t', 'treo logo']): set_sent('PERFORMANCE', 1)

    if has_kw(['giao h√†ng', 'ship', 'v·∫≠n chuy·ªÉn', 'ƒë·∫∑t h√†ng']):
        if has_kw(['nhanh', 'l·∫π', 's·ªõm', 'h·ªèa t·ªëc']): 
            set_sent('SER&ACC', 3)
            pred_vector[ASPECTS.index('PERFORMANCE')] = 0 
        if has_kw(['l√¢u', 'ch·∫≠m', 'l·ªÅ m·ªÅ']): set_sent('SER&ACC', 1)
    
    if has_kw(['ƒë√≥ng g√≥i', 'h·ªôp', 'tai nghe', 's·∫°c']):
        if has_kw(['c·∫©n th·∫≠n', 'ƒë·∫πp', 'k·ªπ']): set_sent('SER&ACC', 3)
        if has_kw(['m√≥p', 'r√°ch', 'thi·∫øu']): set_sent('SER&ACC', 1)

    if has_kw(['nh√¢n vi√™n', 'shop', 't∆∞ v·∫•n']):
        if has_kw(['nhi·ªát t√¨nh', 'd·ªÖ th∆∞∆°ng', 't·ªët']): set_sent('SER&ACC', 3)
        if has_kw(['l·ªìi l√µm', 'th√°i ƒë·ªô', 'b·ªë l√°o']): set_sent('SER&ACC', 1)

    if has_kw(['gi√°', 'ti·ªÅn', 't√∫i ti·ªÅn']):
        if has_kw(['r·∫ª', 't·ªët', 'h·ª£p l√Ω', 'ok', 'ngon']): set_sent('PRICE', 3)
        if has_kw(['ƒë·∫Øt', 'cao', 'ch√°t']): set_sent('PRICE', 1)
    if has_kw(['ƒë√°ng ƒë·ªìng ti·ªÅn', 'ƒë√°ng ti·ªÅn']): set_sent('PRICE', 3)

    if has_kw(['wifi', '4g', 's√≥ng', 'v√¢n tay', 'face id']):
        if has_kw(['y·∫øu', 'k√©m', 'ch·∫≠p ch·ªùn', 'l·ªói']): set_sent('FEATURES', 1)
        if has_kw(['nh·∫°y', 'kh·ªèe', 'cƒÉng']): set_sent('FEATURES', 3)
    if has_kw(['loa', '√¢m thanh']):
        if has_kw(['to', 'hay', 'l·ªõn']): set_sent('FEATURES', 3)
        if has_kw(['b√©', 'nh·ªè', 'r√®']): set_sent('FEATURES', 1)

    if has_kw(['th·∫•t v·ªçng', 'ƒë·ª´ng mua', 'tr√°nh xa', 'ph√≠ ti·ªÅn', 'h·ªëi h·∫≠n']): set_sent('GENERAL', 1)
    if has_kw(['n√™n mua', 'tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 'h√†i l√≤ng', '10 ƒëi·ªÉm']):
        if not any(x == 3 for x in pred_vector): set_sent('GENERAL', 3)

    return pred_vector

# =============================================================================
# 5. GIAO DI·ªÜN STREAMLIT CH√çNH
# =============================================================================
st.sidebar.title("‚öôÔ∏è B·∫£ng ƒëi·ªÅu khi·ªÉn")
uploaded_file = st.sidebar.file_uploader("Upload file Training (CSV)", type=['csv'])

if uploaded_file is not None:
    st.sidebar.success("File ƒë√£ t·∫£i l√™n!")
    if st.sidebar.button("Hu·∫•n luy·ªán M√¥ h√¨nh üöÄ"):
        with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Ensemble..."):
            try:
                vectorizer, models, df_visual = train_model(uploaded_file)
                st.session_state['vectorizer'] = vectorizer
                st.session_state['models'] = models
                st.session_state['df_visual'] = df_visual # L∆∞u data ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
                st.sidebar.success("Hu·∫•n luy·ªán ho√†n t·∫•t!")
            except Exception as e:
                st.sidebar.error(f"C√≥ l·ªói x·∫£y ra: {e}")
else:
    st.sidebar.info("Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

st.title("üì± H·ªá Th·ªëng Ph√¢n T√≠ch C·∫£m X√∫c ƒêi·ªán Tho·∫°i")

# T·∫†O TABS
tab1, tab2 = st.tabs(["üîç Ph√¢n T√≠ch B√¨nh Lu·∫≠n", "üìä Tr·ª±c Quan H√≥a D·ªØ Li·ªáu"])

# --- TAB 1: PH√ÇN T√çCH ---
with tab1:
    col1, col2 = st.columns([2, 1])

    with col1:
        user_input = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n c·ªßa kh√°ch h√†ng:", height=150, placeholder="V√≠ d·ª•: M√°y d√πng t·ªët, pin tr√¢u nh∆∞ng camera h∆°i m·ªù...")
        analyze_btn = st.button("Ph√¢n t√≠ch ngay ‚ú®", type="primary")

    if analyze_btn and user_input:
        if 'models' not in st.session_state:
            st.error("‚ö†Ô∏è Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc!")
        else:
            # Predict
            cleaned_text = clean_text_ultimate(user_input)
            vec_input = st.session_state['vectorizer'].transform([cleaned_text])
            
            ml_preds = []
            for aspect in ASPECTS:
                ml_preds.append(st.session_state['models'][aspect].predict(vec_input)[0])
            
            final_preds = apply_hard_rules_hybrid(user_input, np.array(ml_preds))
            
            # --- T√çNH TO√ÅN K·∫æT LU·∫¨N T·ªîNG QUAN ---
            # L·ªçc c√°c aspect c√≥ nh·∫Øc ƒë·∫øn (kh√°c 0)
            active_sentiments = [p for p in final_preds if p != 0]
            
            st.markdown("---")
            
            if not active_sentiments:
                 st.warning("Kh√¥ng t√¨m th·∫•y kh√≠a c·∫°nh c·ª• th·ªÉ n√†o trong b√¨nh lu·∫≠n.")
            else:
                n_pos = active_sentiments.count(3)
                n_neg = active_sentiments.count(1)
                n_neu = active_sentiments.count(2)

                # Logic k·∫øt lu·∫≠n
                if n_pos > n_neg:
                    overall_html = f"""
                    <div class="overall-card positive">
                        üåü K·∫æT LU·∫¨N: KH√ÅCH H√ÄNG H√ÄI L√íNG<br>
                        <span style="font-size: 16px; font-weight: normal;">(T√≠ch c·ª±c: {n_pos} | Ti√™u c·ª±c: {n_neg})</span>
                    </div>
                    """
                elif n_neg > n_pos:
                    overall_html = f"""
                    <div class="overall-card negative">
                        üò° K·∫æT LU·∫¨N: KH√ÅCH H√ÄNG KH√îNG H√ÄI L√íNG<br>
                        <span style="font-size: 16px; font-weight: normal;">(T√≠ch c·ª±c: {n_pos} | Ti√™u c·ª±c: {n_neg})</span>
                    </div>
                    """
                else:
                    overall_html = f"""
                    <div class="overall-card neutral">
                        ‚öñÔ∏è K·∫æT LU·∫¨N: ƒê√ÅNH GI√Å TRUNG T√çNH / TR√ÅI CHI·ªÄU<br>
                        <span style="font-size: 16px; font-weight: normal;">(T√≠ch c·ª±c: {n_pos} | Ti√™u c·ª±c: {n_neg})</span>
                    </div>
                    """
                
                st.markdown(overall_html, unsafe_allow_html=True)

                # --- HI·ªÇN TH·ªä CHI TI·∫æT T·ª™NG KH√çA C·∫†NH ---
                st.subheader("üìù Chi ti·∫øt ph√¢n t√≠ch:")
                cols = st.columns(4)
                col_idx = 0
                for i, aspect in enumerate(ASPECTS):
                    sentiment = final_preds[i]
                    if sentiment != 0:
                        color_class = "positive" if sentiment == 3 else "negative" if sentiment == 1 else "neutral"
                        label_text = SENTIMENT_MAP[sentiment]
                        with cols[col_idx % 4]:
                            st.markdown(f"""
                            <div class="metric-card {color_class}">
                                <div>{aspect}</div>
                                <div style="font-size: 1.2em;">{label_text}</div>
                            </div>
                            """, unsafe_allow_html=True)
                        col_idx += 1

    with col2:
        st.markdown("### ‚ÑπÔ∏è H∆∞·ªõng d·∫´n")
        st.info("""
        **Quy tr√¨nh:**
        1. T·∫£i file CSV hu·∫•n luy·ªán.
        2. Nh·∫•n n√∫t "Hu·∫•n luy·ªán".
        3. Nh·∫≠p b√¨nh lu·∫≠n v√† xem k·∫øt qu·∫£.
        
        **M√†u s·∫Øc:**
        - üü¢ Xanh: T√≠ch c·ª±c
        - üî¥ ƒê·ªè: Ti√™u c·ª±c
        - üîò X√°m: Trung t√≠nh
        """)
        if 'models' in st.session_state:
            st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")

# --- TAB 2: TR·ª∞C QUAN H√ìA ---
with tab2:
    if 'df_visual' not in st.session_state:
        st.warning("‚ö†Ô∏è Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh ·ªü Tab 'Ph√¢n T√≠ch' ƒë·ªÉ t·∫£i d·ªØ li·ªáu!")
    else:
        df = st.session_state['df_visual']
        st.header("üìä Dashboard Ph√¢n T√≠ch D·ªØ Li·ªáu")
        
        # 1. Ph√¢n ph·ªëi Sao
        st.subheader("1. Ph√¢n ph·ªëi ƒë√°nh gi√° sao (1-5)")
        fig1, ax1 = plt.subplots(figsize=(8, 4))
        if 'n_star' in df.columns:
            sns.countplot(x=df["n_star"], color="#33CCFF", ax=ax1)
            st.pyplot(fig1)
        else:
            st.write("Kh√¥ng t√¨m th·∫•y c·ªôt 'n_star'.")

        # 2. T·ªïng quan Sentiment (Pie Chart)
        st.subheader("2. T·ª∑ l·ªá C·∫£m x√∫c To√†n h·ªá th·ªëng")
        polarity_counts = {
            "Negative": (df[ASPECTS] == 1).sum().sum(),
            "Neutral":  (df[ASPECTS] == 2).sum().sum(),
            "Positive": (df[ASPECTS] == 3).sum().sum(),
        }
        fig2, ax2 = plt.subplots()
        ax2.pie(polarity_counts.values(), labels=polarity_counts.keys(), autopct='%1.1f%%', colors=['#dc3545', '#6c757d', '#28a745'])
        st.pyplot(fig2)

        # 3. Ph√¢n ph·ªëi theo Kh√≠a c·∫°nh (Bar Chart)
        st.subheader("3. Chi ti·∫øt C·∫£m x√∫c theo Kh√≠a c·∫°nh")
        aspect_sentiment = pd.DataFrame({
            "Aspect": ASPECTS,
            "Negative": [(df[a] == 1).sum() for a in ASPECTS],
            "Neutral":  [(df[a] == 2).sum() for a in ASPECTS],
            "Positive": [(df[a] == 3).sum() for a in ASPECTS],
        })
        fig3 = aspect_sentiment.set_index("Aspect").plot(kind="bar", figsize=(12, 6), color=['#dc3545', '#6c757d', '#28a745']).figure
        st.pyplot(fig3)

        # 4. Heatmap T∆∞∆°ng quan (Correlation)
        st.subheader("4. Ma tr·∫≠n T∆∞∆°ng quan gi·ªØa c√°c Kh√≠a c·∫°nh")
        corr = df[ASPECTS].replace({0: np.nan}).corr()
        fig4, ax4 = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr, annot=True, cmap="coolwarm", ax=ax4)
        st.pyplot(fig4)

        # 5. WordCloud
        st.subheader("5. T·ª´ kh√≥a n·ªïi b·∫≠t (WordCloud)")
        col_wc1, col_wc2 = st.columns(2)
        
        positive_text = " ".join(df[df[ASPECTS].eq(3).any(axis=1)]["comment_cleaned"])
        negative_text = " ".join(df[df[ASPECTS].eq(1).any(axis=1)]["comment_cleaned"])
        
        with col_wc1:
            st.write("**T·ª´ kh√≥a T√≠ch c·ª±c**")
            if len(positive_text) > 0:
                wc_pos = WordCloud(width=400, height=300, background_color="white").generate(positive_text)
                fig_p, ax_p = plt.subplots()
                ax_p.imshow(wc_pos, interpolation='bilinear')
                ax_p.axis("off")
                st.pyplot(fig_p)
        
        with col_wc2:
            st.write("**T·ª´ kh√≥a Ti√™u c·ª±c**")
            if len(negative_text) > 0:
                wc_neg = WordCloud(width=400, height=300, background_color="white", colormap="Reds").generate(negative_text)
                fig_n, ax_n = plt.subplots()
                ax_n.imshow(wc_neg, interpolation='bilinear')
                ax_n.axis("off")

                st.pyplot(fig_n)
